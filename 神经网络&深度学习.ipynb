{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>4维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(10,1,28,28)  #生成10个数据，每个数据的形状为（1,28,28）\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.63432864e-01, 6.52503544e-01, 6.43321813e-01, 3.31604351e-01,\n",
       "        8.25106361e-01, 5.63306407e-02, 4.45056781e-01, 4.76298329e-01,\n",
       "        5.28364553e-02, 7.90381672e-01, 9.34983295e-01, 9.52754012e-01,\n",
       "        6.27787096e-01, 6.14799367e-01, 4.80650744e-01, 1.84763000e-01,\n",
       "        4.81712619e-01, 3.14580158e-01, 9.20279126e-01, 7.51396880e-01,\n",
       "        2.90735485e-01, 2.43292236e-01, 6.15137519e-01, 4.09764076e-01,\n",
       "        5.97808862e-01, 4.44321323e-01, 6.76665911e-01, 3.65545384e-01],\n",
       "       [3.25819667e-01, 1.73675500e-01, 7.46450938e-01, 2.56943854e-02,\n",
       "        5.64064205e-01, 6.53597859e-02, 5.82177378e-01, 3.15878450e-01,\n",
       "        9.07952078e-01, 4.83139399e-01, 6.11471044e-01, 2.85470254e-01,\n",
       "        3.72182894e-01, 5.27854290e-01, 2.65869593e-02, 7.00046724e-01,\n",
       "        7.49932661e-01, 8.42906821e-01, 3.80925481e-01, 7.93140262e-01,\n",
       "        8.97365738e-01, 1.59303614e-01, 2.43288165e-01, 9.68417812e-01,\n",
       "        2.97129677e-01, 7.53532417e-02, 1.66809852e-01, 1.82425210e-01],\n",
       "       [7.69243403e-02, 5.64326852e-01, 7.56469267e-01, 4.49910831e-01,\n",
       "        4.70746712e-01, 4.71483879e-01, 9.85546030e-02, 3.66067210e-01,\n",
       "        6.06719869e-01, 9.39089212e-01, 4.27111231e-01, 6.43707903e-01,\n",
       "        5.53335076e-01, 5.74202046e-01, 1.46262397e-01, 8.67579361e-01,\n",
       "        3.39647150e-02, 7.64141789e-01, 9.86335512e-01, 5.57911051e-01,\n",
       "        5.11712038e-01, 1.69135304e-02, 7.45713670e-01, 9.07428100e-01,\n",
       "        5.79819348e-01, 2.83609952e-01, 1.57579481e-01, 2.98029940e-02],\n",
       "       [4.08503116e-01, 1.14171900e-02, 8.99530507e-01, 3.72677894e-02,\n",
       "        6.83926634e-01, 1.70253010e-01, 4.28823798e-01, 3.76842497e-01,\n",
       "        7.34331451e-02, 3.80882356e-01, 5.87061129e-01, 6.20324242e-01,\n",
       "        6.10954987e-01, 4.24350608e-01, 3.10821752e-01, 3.43835964e-01,\n",
       "        1.02011223e-01, 1.27357924e-01, 8.63376104e-01, 5.12455219e-02,\n",
       "        5.19557589e-01, 5.61188563e-01, 1.26132900e-02, 8.71495638e-01,\n",
       "        8.65172938e-01, 4.06959166e-01, 6.75391172e-01, 2.82712323e-02],\n",
       "       [7.27870010e-01, 7.46862081e-01, 8.86478533e-01, 6.52898474e-03,\n",
       "        4.12947070e-02, 6.73203736e-01, 7.71410271e-01, 9.73803670e-01,\n",
       "        3.72473288e-01, 7.23227969e-01, 1.61209995e-01, 6.89942785e-01,\n",
       "        5.37568193e-02, 2.58429006e-01, 2.07848508e-01, 4.11833501e-01,\n",
       "        2.16650405e-01, 1.00905398e-01, 4.53743167e-01, 6.40533592e-02,\n",
       "        9.89590263e-01, 8.52011608e-01, 2.58666409e-01, 8.21898239e-01,\n",
       "        9.88292083e-01, 5.58074928e-01, 9.66520052e-01, 6.94351195e-01],\n",
       "       [1.28730398e-02, 5.14759241e-01, 3.27779177e-01, 4.30448231e-01,\n",
       "        3.58464308e-02, 5.08553936e-01, 3.80901222e-01, 2.33672290e-01,\n",
       "        1.30903482e-01, 7.85842665e-01, 2.51783545e-01, 6.44154158e-01,\n",
       "        1.64727752e-01, 3.95092209e-01, 3.86817252e-01, 8.60319694e-01,\n",
       "        7.49203656e-01, 1.43935045e-01, 1.44574886e-01, 7.14516128e-01,\n",
       "        6.52976616e-02, 9.54956370e-01, 1.32379621e-01, 1.72741796e-01,\n",
       "        9.55159974e-01, 7.62390833e-02, 8.88572815e-01, 6.03105518e-01],\n",
       "       [7.32872698e-01, 6.92460285e-01, 7.39395037e-01, 3.11011839e-01,\n",
       "        3.08808436e-01, 4.06317614e-01, 9.93577912e-01, 3.08361473e-01,\n",
       "        7.79036661e-01, 7.83183040e-01, 9.94706627e-01, 8.35545578e-01,\n",
       "        7.13576184e-01, 4.50105356e-02, 3.31766966e-01, 4.52928440e-01,\n",
       "        6.82612063e-01, 9.17484334e-01, 9.73076182e-01, 8.61142841e-01,\n",
       "        6.82578276e-01, 8.67622058e-01, 5.52935137e-01, 5.42037636e-01,\n",
       "        9.98767708e-01, 4.60504240e-01, 7.74546907e-01, 7.69677270e-01],\n",
       "       [1.93063591e-01, 5.82687712e-01, 8.66433593e-01, 2.35546434e-01,\n",
       "        1.21942845e-01, 2.58623067e-01, 2.73780982e-01, 2.80262233e-01,\n",
       "        8.28436112e-01, 7.84181636e-04, 5.89280833e-01, 3.53631765e-01,\n",
       "        3.02624785e-01, 8.85801232e-01, 7.91632181e-01, 1.81038326e-01,\n",
       "        9.53554136e-01, 3.58360461e-01, 9.91432654e-01, 5.69457667e-01,\n",
       "        7.64367117e-01, 8.54059801e-01, 4.21680474e-01, 1.61075319e-01,\n",
       "        7.73593526e-01, 4.70708164e-01, 3.66914258e-01, 1.48782799e-01],\n",
       "       [8.51270198e-01, 2.53199195e-01, 9.76069125e-02, 6.43773392e-01,\n",
       "        7.41257553e-01, 4.13747020e-01, 7.26266836e-01, 4.46822999e-03,\n",
       "        2.77883072e-01, 6.94253485e-01, 2.30273705e-01, 9.74144155e-01,\n",
       "        6.27490404e-02, 4.54573597e-01, 2.19733185e-01, 3.94497217e-01,\n",
       "        3.45381463e-01, 7.42177652e-01, 9.27749177e-01, 2.24360297e-02,\n",
       "        7.09165345e-01, 2.18437369e-01, 7.43338197e-01, 6.21662635e-01,\n",
       "        9.81964538e-01, 5.00812488e-01, 9.22977787e-01, 9.13151421e-01],\n",
       "       [8.29847561e-01, 2.88361894e-02, 8.16610820e-01, 9.03092917e-01,\n",
       "        1.74429255e-01, 8.44332670e-01, 5.72049352e-01, 6.47442550e-02,\n",
       "        3.71203168e-01, 4.76858192e-01, 9.78966856e-01, 2.47992409e-01,\n",
       "        6.71254494e-01, 4.07594529e-01, 8.65246591e-01, 5.66192498e-01,\n",
       "        4.52021033e-01, 9.16112715e-01, 5.35153633e-01, 8.43850661e-01,\n",
       "        4.37731904e-01, 7.42400985e-01, 9.00893211e-01, 2.87805588e-01,\n",
       "        6.70996050e-02, 6.08301627e-01, 5.88891081e-01, 3.31774775e-01],\n",
       "       [7.07928878e-01, 6.05246422e-01, 4.62771786e-01, 4.30787034e-01,\n",
       "        8.21102316e-01, 7.42877911e-01, 5.30260137e-01, 1.25670465e-01,\n",
       "        2.40273593e-01, 4.07502445e-01, 5.49505052e-02, 6.52441529e-01,\n",
       "        1.66175183e-01, 3.16441519e-01, 2.92539150e-01, 9.28398477e-01,\n",
       "        8.56406119e-01, 8.84735087e-02, 6.46930682e-02, 3.91019992e-01,\n",
       "        2.61889964e-01, 6.20787535e-01, 1.33271249e-01, 6.46266656e-01,\n",
       "        9.03108828e-01, 8.35599087e-01, 9.18190851e-03, 8.39220481e-01],\n",
       "       [6.28954333e-01, 5.61738305e-01, 2.23594068e-01, 2.08295724e-01,\n",
       "        6.97809468e-01, 8.38667892e-01, 4.73082957e-01, 8.12349334e-01,\n",
       "        4.21140350e-01, 9.44280386e-01, 3.09570608e-01, 9.24493776e-01,\n",
       "        4.32569828e-01, 6.64189622e-01, 6.18166724e-01, 3.84689836e-01,\n",
       "        9.22894462e-02, 6.75326676e-01, 6.36534109e-01, 9.47865226e-01,\n",
       "        9.69422992e-01, 6.41705656e-01, 3.40021010e-01, 5.48101453e-01,\n",
       "        7.43375439e-01, 3.40600299e-01, 9.94115366e-01, 6.43200605e-01],\n",
       "       [3.22526261e-01, 6.31138682e-01, 9.73364110e-01, 9.48293691e-02,\n",
       "        7.79602945e-01, 3.19279029e-01, 9.84748749e-01, 3.00466206e-01,\n",
       "        8.26396379e-01, 1.37521654e-01, 4.22870642e-01, 8.71789886e-01,\n",
       "        8.03764305e-01, 7.29102968e-01, 6.25288014e-01, 4.73121842e-01,\n",
       "        7.08514324e-01, 6.04921011e-01, 3.21758036e-02, 3.75593369e-01,\n",
       "        6.11013001e-01, 6.83044577e-01, 3.27563486e-01, 4.52953271e-01,\n",
       "        2.31920816e-02, 8.43329841e-01, 7.26213533e-01, 5.41188033e-01],\n",
       "       [8.46833927e-01, 6.62640192e-01, 1.87703616e-01, 2.98067526e-01,\n",
       "        1.72533587e-01, 4.41881496e-01, 9.18633025e-01, 6.76685644e-01,\n",
       "        3.67006013e-01, 6.92339444e-01, 2.84382035e-01, 9.75418932e-01,\n",
       "        8.99842920e-02, 3.99422478e-01, 3.04852472e-01, 3.00812141e-01,\n",
       "        6.73223511e-01, 6.05934216e-01, 5.48311623e-01, 5.81907002e-01,\n",
       "        3.30451345e-01, 7.87821082e-01, 8.15614002e-01, 3.15497221e-01,\n",
       "        4.25744250e-01, 3.60043384e-01, 9.42519757e-01, 5.56953807e-01],\n",
       "       [3.57565093e-01, 5.59249627e-01, 9.14827906e-01, 3.94198407e-01,\n",
       "        1.17820705e-01, 7.68475686e-01, 1.77873325e-01, 2.48107529e-01,\n",
       "        9.59580214e-01, 2.96535565e-01, 5.19742795e-01, 6.27354026e-01,\n",
       "        2.44188695e-01, 2.39098390e-01, 2.17601899e-01, 3.43579605e-01,\n",
       "        3.25202590e-01, 5.48151616e-01, 2.65167414e-01, 3.48434781e-01,\n",
       "        8.88448187e-01, 8.68794230e-01, 4.04030417e-02, 6.19706005e-01,\n",
       "        1.10235882e-01, 7.97261483e-02, 8.30932038e-01, 8.98465078e-01],\n",
       "       [8.19635299e-02, 8.79770039e-01, 1.74110480e-01, 5.75843756e-01,\n",
       "        5.55800444e-01, 6.64048484e-02, 8.46239978e-01, 7.17199569e-01,\n",
       "        1.70996397e-01, 7.02234238e-01, 2.36131366e-01, 4.62602113e-02,\n",
       "        1.62796228e-01, 3.88886638e-01, 8.97150344e-01, 5.64703981e-01,\n",
       "        3.63286340e-01, 6.36688000e-01, 3.58391091e-01, 2.62077122e-01,\n",
       "        6.44635797e-01, 2.13731335e-01, 1.21304222e-01, 7.62836475e-01,\n",
       "        6.55908108e-01, 6.71562604e-01, 9.75953180e-01, 7.52292323e-01],\n",
       "       [5.46895460e-01, 4.53667666e-01, 9.55148338e-01, 6.76383650e-01,\n",
       "        8.14801642e-01, 1.40437237e-02, 4.13278984e-02, 7.52144792e-01,\n",
       "        3.47872071e-01, 8.39170088e-01, 5.00491624e-02, 9.29160359e-01,\n",
       "        3.78364491e-01, 1.74630472e-01, 5.57681313e-01, 9.24754267e-01,\n",
       "        3.08971223e-01, 8.75209021e-01, 3.86177946e-01, 1.61480491e-01,\n",
       "        4.42001855e-01, 6.09863454e-01, 6.89751600e-01, 7.16464789e-01,\n",
       "        4.23452576e-01, 7.13370920e-01, 1.61089352e-01, 8.09239457e-01],\n",
       "       [8.27850743e-01, 8.79283311e-01, 8.13105973e-01, 7.81764692e-01,\n",
       "        3.38860720e-01, 5.10530502e-02, 3.31022157e-01, 1.26205996e-01,\n",
       "        3.78165578e-01, 3.62406084e-01, 9.99586418e-01, 2.76220852e-01,\n",
       "        1.18475153e-01, 5.91669211e-01, 1.46031119e-01, 3.02265047e-01,\n",
       "        4.22896586e-01, 9.29960755e-02, 9.26199966e-01, 8.34593795e-03,\n",
       "        7.99167370e-01, 8.95183647e-02, 7.73773168e-02, 7.47854075e-01,\n",
       "        9.02675939e-01, 6.68381056e-01, 8.65839585e-01, 8.13962685e-01],\n",
       "       [3.44425612e-01, 4.93938737e-01, 3.85814300e-01, 1.91360065e-01,\n",
       "        8.71274868e-01, 2.82643710e-01, 6.16552296e-01, 7.24619665e-02,\n",
       "        3.39290646e-01, 9.80960559e-01, 8.02926231e-01, 3.82583146e-01,\n",
       "        4.90059027e-01, 8.55869904e-02, 8.08295976e-01, 5.10374338e-01,\n",
       "        9.40280207e-01, 9.87513302e-01, 5.74031777e-01, 1.71460853e-01,\n",
       "        5.96255385e-01, 4.22876518e-01, 8.72244441e-01, 2.99862141e-02,\n",
       "        9.81680331e-02, 1.19528100e-01, 3.62828944e-01, 8.15051642e-01],\n",
       "       [3.18607959e-01, 2.29476808e-01, 6.40602741e-01, 7.95968248e-02,\n",
       "        6.70776903e-01, 2.39996604e-01, 9.98467369e-01, 7.78775140e-01,\n",
       "        8.87703673e-01, 8.03930004e-01, 6.57230307e-01, 7.88748808e-01,\n",
       "        8.27398280e-01, 3.16800771e-01, 2.46499249e-01, 9.86125946e-01,\n",
       "        7.51726335e-01, 7.76938509e-01, 6.68280094e-01, 2.51005498e-01,\n",
       "        8.37525589e-01, 8.78927541e-01, 3.73087514e-01, 8.51002687e-01,\n",
       "        7.14164963e-01, 8.58621667e-01, 6.75266280e-01, 4.11227721e-01],\n",
       "       [9.60096966e-01, 2.18340747e-02, 2.07298760e-01, 6.87142659e-01,\n",
       "        1.31973819e-01, 6.93647270e-01, 1.72706691e-01, 8.95703542e-01,\n",
       "        9.58696079e-01, 5.70934920e-01, 6.54999680e-01, 2.53089658e-01,\n",
       "        3.44920315e-01, 8.21695143e-01, 4.60968111e-01, 5.46470706e-01,\n",
       "        9.57697985e-01, 8.39918922e-01, 2.39911761e-01, 2.37592799e-01,\n",
       "        4.81687082e-01, 5.21280430e-03, 2.63486647e-01, 6.37523205e-01,\n",
       "        1.97395491e-01, 4.62488959e-01, 3.78461614e-01, 1.93587477e-02],\n",
       "       [4.57200841e-01, 5.25253156e-01, 5.10848394e-01, 9.56228960e-01,\n",
       "        2.63298213e-01, 8.53330501e-01, 8.33427483e-01, 8.32778392e-01,\n",
       "        1.35733472e-01, 6.29258342e-01, 4.62378960e-01, 4.90702673e-02,\n",
       "        5.65277470e-01, 3.43353110e-02, 1.48194806e-01, 6.33252575e-01,\n",
       "        9.95119335e-01, 9.14381748e-01, 8.38209158e-01, 2.52213205e-01,\n",
       "        9.07990728e-01, 3.53228351e-01, 2.81141699e-02, 3.07817119e-01,\n",
       "        3.11799429e-01, 6.52237291e-01, 5.01360082e-01, 4.13542973e-01],\n",
       "       [3.02530864e-01, 2.27600677e-01, 7.47708662e-01, 7.10540363e-02,\n",
       "        6.96994342e-01, 2.36700483e-01, 4.42714949e-01, 3.18211275e-01,\n",
       "        9.73450684e-01, 9.74937715e-01, 2.53676784e-01, 3.40404159e-01,\n",
       "        6.15742056e-01, 2.25330083e-01, 3.15084219e-01, 2.56207792e-01,\n",
       "        6.59578459e-01, 1.98315695e-01, 7.85511155e-01, 3.97244245e-01,\n",
       "        4.40453362e-01, 2.93650170e-01, 5.03228032e-01, 3.63701552e-01,\n",
       "        7.08397444e-01, 7.96984871e-01, 9.11416856e-02, 1.22536019e-01],\n",
       "       [6.90392624e-01, 7.05671615e-02, 3.97430001e-01, 5.90769155e-01,\n",
       "        2.80375425e-01, 1.47841879e-01, 8.80484672e-01, 2.40409204e-01,\n",
       "        9.84163408e-01, 3.98060882e-01, 3.42238007e-01, 7.36990992e-01,\n",
       "        5.76017850e-02, 7.47033724e-01, 5.49271182e-01, 9.41490992e-01,\n",
       "        1.41397429e-01, 3.80093813e-01, 5.64213385e-01, 7.17070591e-01,\n",
       "        4.43295505e-01, 6.84972894e-01, 5.65087402e-01, 9.02628562e-01,\n",
       "        4.32757366e-01, 2.46255413e-02, 4.62606453e-01, 8.97413392e-01],\n",
       "       [7.83645670e-01, 5.97706660e-01, 6.86301018e-01, 6.82685474e-01,\n",
       "        3.23917574e-01, 2.12707316e-01, 9.88528035e-01, 5.58721055e-01,\n",
       "        2.11389776e-01, 1.32366207e-01, 7.56805715e-01, 1.79489375e-01,\n",
       "        5.53521012e-01, 7.31209354e-01, 3.96178020e-01, 5.79584850e-01,\n",
       "        8.32377853e-01, 1.02330763e-01, 7.50426001e-01, 4.46937118e-01,\n",
       "        7.29412871e-01, 4.52659628e-01, 9.17317143e-01, 6.25524397e-01,\n",
       "        3.79383907e-01, 1.78179998e-01, 5.36498797e-01, 6.01699114e-01],\n",
       "       [1.81321061e-01, 3.72571606e-02, 3.54344077e-02, 4.30114440e-01,\n",
       "        6.29234848e-01, 1.55652443e-01, 5.46576801e-01, 1.05460492e-01,\n",
       "        8.30195588e-01, 7.08669277e-01, 4.18289264e-01, 2.84711577e-01,\n",
       "        6.29889488e-01, 7.38677066e-01, 5.09300034e-01, 6.38190702e-01,\n",
       "        2.83556275e-01, 7.03331903e-01, 8.89541263e-01, 6.19768619e-01,\n",
       "        5.82445432e-01, 5.38866986e-01, 8.95232828e-01, 5.16672841e-01,\n",
       "        3.01427542e-01, 8.93695960e-01, 3.92654211e-01, 5.03283888e-01],\n",
       "       [9.75986576e-01, 9.79051433e-01, 7.91524140e-01, 3.16894008e-01,\n",
       "        9.71816822e-01, 5.95906123e-01, 9.61862638e-01, 2.24469535e-01,\n",
       "        5.11645268e-01, 7.03542555e-01, 6.62945260e-01, 8.12159957e-01,\n",
       "        9.39062267e-01, 6.37429723e-02, 1.25585912e-01, 8.07544506e-01,\n",
       "        2.25304504e-01, 9.80369423e-01, 1.94096623e-01, 1.33115468e-01,\n",
       "        4.39328498e-01, 4.23299979e-01, 8.26516794e-02, 5.49438030e-02,\n",
       "        9.73869307e-01, 9.14368092e-02, 7.96554551e-02, 6.83937800e-01],\n",
       "       [5.83643647e-02, 7.14664476e-01, 4.21267922e-01, 8.21022995e-01,\n",
       "        3.27839950e-01, 2.79427123e-01, 7.97519206e-01, 6.51572708e-01,\n",
       "        9.83815090e-01, 6.01887073e-01, 7.30407253e-01, 4.42385119e-01,\n",
       "        3.07519908e-02, 1.89131815e-01, 2.70579633e-01, 4.84750388e-01,\n",
       "        5.47932168e-01, 6.14670221e-01, 9.11369602e-01, 4.02388857e-01,\n",
       "        8.18351108e-02, 6.56267873e-01, 9.82890168e-01, 8.08710179e-01,\n",
       "        6.39130973e-01, 3.49443577e-02, 4.56127346e-02, 7.17679693e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#访问第一个数据的第一个通道的空间数据\n",
    "x[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>定义softmax函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)  #防止溢出\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>定义cross_entropy_error函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    if t.size == y.size:\n",
    "        error = -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "    else:\n",
    "        error = -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7)) / batch_size\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>回归问题使用恒等函数，分类问题使用softmax函数，softmax函数对应使用交叉熵误差计算损失函数，恒等函数对应使用平方和误差计算损失函数，这样在反向传播时可以得到（yi-ti）这样的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>定义ReLU层的类：Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 #将小于0的数转化为0\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>定义Sigmoid层的类：Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout * (1 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>定义Affine层的类：Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        self.x = x\n",
    "        \n",
    "        out = np.dot(self.x,self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape) #还原输入数据的形状\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>定义Softmax-with-Loss层的类：SoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None  #softmax的输出\n",
    "        self.t = None  #监督数据\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size:   #监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t)/batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size),self.t] -= 1\n",
    "            dx = dx/batch_size\n",
    "            \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 face='黑体'>softmax函数和cross_entropy_error函数见上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im2col函数：将3维或4维数据转换为2维矩阵\n",
    "import numpy as np\n",
    "from common.util import im2col\n",
    "from common.util import col2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1,5,5,stride=1,pad=0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>定义卷积运算的类：Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self,W,b,stride=1,pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        FN,C,FH,FW = self.W.shape\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = int(1+(H + 2*self.pad - FH)/self.stride)\n",
    "        out_w = int(1+(W + 2*self.pad - FW)/self.stride)\n",
    "        \n",
    "        col = im2col(x,FH,FW,self.stride,self.pad)\n",
    "        col_W = self.W.reshape(FN,-1).T\n",
    "        \n",
    "        out = np.dot(col,col_W)+self.b\n",
    "        out = out.reshape(N,out_h,out_w,-1).transpose(0,3,1,2)  #transpose：改变多维数组轴的顺序\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>backward中只有Affine层和Convolution层计算参数的梯度，其他的层（包括Affine和Convolution）进行数据的传递"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>定义池化的类：Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self,pool_h,pool_w,stride=1,pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = int(1+(H-self.pool_h)/self.stride)\n",
    "        out_w = int(1+(W-self.pool_w)/self.stride)\n",
    "        \n",
    "        #展开\n",
    "        col = im2col(x,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "        col = col.reshape(-1,self.pool_h * self.pool_w)\n",
    "        \n",
    "        #取最大值\n",
    "        out = np.max(col,axis=1)\n",
    "        \n",
    "        #转换\n",
    "        out = out.reshape(N,out_h,out_w,C).transpose(0,3,1,2)\n",
    "        \n",
    "        arg_max = np.argmax(col,axis=1)\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>CNN的实现(创建一个简单的卷积神经网络（上面各个层的拼接）来学习手写数字图像识别)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pickle\n",
    "from common.layers import *\n",
    "import numpy as np\n",
    "from common.util import im2col\n",
    "from common.util import col2im\n",
    "\n",
    "class SimpleConvNet:\n",
    "    #初始化部分\n",
    "    def __init__(self,input_dim=(1,28,28),\n",
    "                 conv_param={'filter_num':30,'filter_size':5,'pad':0,'stride':1},\n",
    "                hidden_size=100,output_size=10,weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad)/filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))  \n",
    "        #因为下文设定的Pooling层的size=2，stride=2，把Pooling层的输出展开为一行，才可以进行下面的Affine层\n",
    "        \n",
    "        #参数的初始化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num,input_dim[0],filter_size,filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size,hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        #生成必要的层\n",
    "        self.layers = OrderedDict()   #生成有序字典\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],self.params['b1'],conv_param['stride'],conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2,pool_w=2,stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'],self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'],self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    #进行预测与计算损失函数\n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    #最后基于误差反向传播法求梯度\n",
    "    def gradient(self,x,t):\n",
    "        #forward\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        #设定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red face='黑体'>使用这个SimpleConvNet来学习MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2995897386427537\n",
      "=== epoch:1, train acc:0.286, test acc:0.298 ===\n",
      "train loss:2.298181725397648\n",
      "train loss:2.2926647930895343\n",
      "train loss:2.2891189497689965\n",
      "train loss:2.281954914680036\n",
      "train loss:2.2708708539408873\n",
      "train loss:2.247008451106033\n",
      "train loss:2.2458318689962598\n",
      "train loss:2.214124017944188\n",
      "train loss:2.1884758260231805\n",
      "train loss:2.146348235498979\n",
      "train loss:2.1076805738665154\n",
      "train loss:2.094495635157899\n",
      "train loss:2.0290312653670357\n",
      "train loss:2.0821116903678534\n",
      "train loss:1.9489001707835039\n",
      "train loss:1.9055595840417832\n",
      "train loss:1.838576732567927\n",
      "train loss:1.7355329508718733\n",
      "train loss:1.5971692878242454\n",
      "train loss:1.594581318542135\n",
      "train loss:1.4742619381475703\n",
      "train loss:1.3398282705972082\n",
      "train loss:1.3389788684273944\n",
      "train loss:1.245954355632327\n",
      "train loss:1.2253547110316914\n",
      "train loss:1.0605415218756171\n",
      "train loss:0.8998996379568663\n",
      "train loss:0.8864097897525731\n",
      "train loss:0.8044214645190374\n",
      "train loss:0.9486363317659784\n",
      "train loss:0.7415725089518904\n",
      "train loss:0.7311036192306446\n",
      "train loss:0.9242274372925197\n",
      "train loss:0.6351377613194965\n",
      "train loss:0.5099224679807056\n",
      "train loss:0.5372966974980313\n",
      "train loss:0.46510598107378576\n",
      "train loss:0.5896026110957314\n",
      "train loss:0.5029142629283081\n",
      "train loss:0.5440192113828344\n",
      "train loss:0.6001544299919455\n",
      "train loss:0.47157777507617965\n",
      "train loss:0.5235186542767617\n",
      "train loss:0.5468545105622118\n",
      "train loss:0.5640146159159266\n",
      "train loss:0.4866599789326392\n",
      "train loss:0.4825477549020178\n",
      "train loss:0.7814563525239072\n",
      "train loss:0.568424417270167\n",
      "train loss:0.6639364424907782\n",
      "=== epoch:2, train acc:0.803, test acc:0.804 ===\n",
      "train loss:0.45543271333109286\n",
      "train loss:0.44916102759598187\n",
      "train loss:0.5727954909176219\n",
      "train loss:0.5638192681360418\n",
      "train loss:0.3873654827370118\n",
      "train loss:0.8061235252180601\n",
      "train loss:0.4412041121221833\n",
      "train loss:0.543869336317168\n",
      "train loss:0.6055323436764256\n",
      "train loss:0.5549742335445779\n",
      "train loss:0.6050177325008723\n",
      "train loss:0.4109420316875792\n",
      "train loss:0.4468740328967988\n",
      "train loss:0.47657883582295923\n",
      "train loss:0.44435108411144997\n",
      "train loss:0.5786965766103451\n",
      "train loss:0.41641552586596353\n",
      "train loss:0.3983974282917739\n",
      "train loss:0.3956061470529429\n",
      "train loss:0.3938187585786768\n",
      "train loss:0.33394663835654415\n",
      "train loss:0.4654575277822763\n",
      "train loss:0.3176650605485348\n",
      "train loss:0.4159161039095033\n",
      "train loss:0.3458714965106968\n",
      "train loss:0.4589760952143335\n",
      "train loss:0.5363981701507252\n",
      "train loss:0.4369273134950803\n",
      "train loss:0.5332292491393291\n",
      "train loss:0.378318213410999\n",
      "train loss:0.48705598614490225\n",
      "train loss:0.37110690051294987\n",
      "train loss:0.5191087770879862\n",
      "train loss:0.48565671087687073\n",
      "train loss:0.2332349414827596\n",
      "train loss:0.5240751755142333\n",
      "train loss:0.35774314032502724\n",
      "train loss:0.31144977899720255\n",
      "train loss:0.344606070497902\n",
      "train loss:0.4369502862963736\n",
      "train loss:0.37062036549936145\n",
      "train loss:0.32209025155018645\n",
      "train loss:0.4309700909437397\n",
      "train loss:0.5546553772129743\n",
      "train loss:0.38015069530369855\n",
      "train loss:0.31610563180896883\n",
      "train loss:0.46483758117597235\n",
      "train loss:0.4088060791927012\n",
      "train loss:0.17859737245344864\n",
      "train loss:0.22762663905212988\n",
      "=== epoch:3, train acc:0.878, test acc:0.866 ===\n",
      "train loss:0.31173445108647296\n",
      "train loss:0.4448489719230054\n",
      "train loss:0.2717710880879229\n",
      "train loss:0.26337943803786196\n",
      "train loss:0.11277759712478586\n",
      "train loss:0.3748380165922361\n",
      "train loss:0.2713733322616471\n",
      "train loss:0.3656021784084018\n",
      "train loss:0.3258362257282989\n",
      "train loss:0.2659115418562573\n",
      "train loss:0.2345245249449247\n",
      "train loss:0.3158213165455227\n",
      "train loss:0.3437553389527438\n",
      "train loss:0.20050073627298956\n",
      "train loss:0.2801072214901922\n",
      "train loss:0.21056586565821053\n",
      "train loss:0.2546150147294953\n",
      "train loss:0.27325144404037843\n",
      "train loss:0.2654905922109851\n",
      "train loss:0.22501174002747656\n",
      "train loss:0.3206704337889816\n",
      "train loss:0.2064499342454853\n",
      "train loss:0.4226625015786835\n",
      "train loss:0.25930092186243064\n",
      "train loss:0.3080814755239753\n",
      "train loss:0.28230940903647234\n",
      "train loss:0.2502244099073916\n",
      "train loss:0.2846786068270742\n",
      "train loss:0.3175437584649232\n",
      "train loss:0.2120760667139825\n",
      "train loss:0.3623106922683265\n",
      "train loss:0.5856490727145062\n",
      "train loss:0.3717999804474665\n",
      "train loss:0.3313902593973675\n",
      "train loss:0.3016623993340023\n",
      "train loss:0.46206005534035727\n",
      "train loss:0.16201665496375628\n",
      "train loss:0.465608391090136\n",
      "train loss:0.29313090704856204\n",
      "train loss:0.2879798458565707\n",
      "train loss:0.2549160159729539\n",
      "train loss:0.15913058066726665\n",
      "train loss:0.2637990483794267\n",
      "train loss:0.18257596076885027\n",
      "train loss:0.21353347494099495\n",
      "train loss:0.3976764035009475\n",
      "train loss:0.47186508049132925\n",
      "train loss:0.21214317236192534\n",
      "train loss:0.3759848322594304\n",
      "train loss:0.22482281373976976\n",
      "=== epoch:4, train acc:0.895, test acc:0.878 ===\n",
      "train loss:0.3022185501974919\n",
      "train loss:0.20438977695166924\n",
      "train loss:0.3512188846690094\n",
      "train loss:0.14179629033047841\n",
      "train loss:0.2683771982648054\n",
      "train loss:0.33060843609782636\n",
      "train loss:0.23892027715034037\n",
      "train loss:0.207807732700044\n",
      "train loss:0.27988864239801875\n",
      "train loss:0.2444829266490505\n",
      "train loss:0.2798362695870013\n",
      "train loss:0.23434766406159283\n",
      "train loss:0.23116606155682745\n",
      "train loss:0.1608536498553029\n",
      "train loss:0.20238737445108046\n",
      "train loss:0.33445851698588747\n",
      "train loss:0.2401712641717886\n",
      "train loss:0.264150964185959\n",
      "train loss:0.24370296584438733\n",
      "train loss:0.2262120483025913\n",
      "train loss:0.2139503271194057\n",
      "train loss:0.3246954082941421\n",
      "train loss:0.3031679838210913\n",
      "train loss:0.40624470328122575\n",
      "train loss:0.23950795292859994\n",
      "train loss:0.4005629403618449\n",
      "train loss:0.2903727509200033\n",
      "train loss:0.18914213637612182\n",
      "train loss:0.22418462521679236\n",
      "train loss:0.13592902720439626\n",
      "train loss:0.22786091312884804\n",
      "train loss:0.27088653294548015\n",
      "train loss:0.2741672231842653\n",
      "train loss:0.3222278979153962\n",
      "train loss:0.1650240343432387\n",
      "train loss:0.22106451968211016\n",
      "train loss:0.2937958784250191\n",
      "train loss:0.28052697461284176\n",
      "train loss:0.26356243963604176\n",
      "train loss:0.22408622754164081\n",
      "train loss:0.17835128377736204\n",
      "train loss:0.42896554143696064\n",
      "train loss:0.21737490389879788\n",
      "train loss:0.27064245986332336\n",
      "train loss:0.16415769369413277\n",
      "train loss:0.11005619322659474\n",
      "train loss:0.25332006811181446\n",
      "train loss:0.2552367503909151\n",
      "train loss:0.14888147817132408\n",
      "train loss:0.29957316017349295\n",
      "=== epoch:5, train acc:0.92, test acc:0.91 ===\n",
      "train loss:0.2362542783811396\n",
      "train loss:0.20972503877695473\n",
      "train loss:0.24598235062274998\n",
      "train loss:0.13564918476697532\n",
      "train loss:0.21314936977285548\n",
      "train loss:0.20919669171646438\n",
      "train loss:0.179080982685265\n",
      "train loss:0.21524326090862156\n",
      "train loss:0.2978343961734391\n",
      "train loss:0.2529551685411621\n",
      "train loss:0.21654569512752744\n",
      "train loss:0.21211704939740814\n",
      "train loss:0.22056927424835276\n",
      "train loss:0.11259537963757808\n",
      "train loss:0.2709047847891023\n",
      "train loss:0.1683898982266564\n",
      "train loss:0.25975447638811394\n",
      "train loss:0.30262843993562166\n",
      "train loss:0.2573745975849364\n",
      "train loss:0.17559752274696946\n",
      "train loss:0.2893211205407583\n",
      "train loss:0.215357753347486\n",
      "train loss:0.29369022497241704\n",
      "train loss:0.38458630299862323\n",
      "train loss:0.13300770650512445\n",
      "train loss:0.08130560524453191\n",
      "train loss:0.2650474863191945\n",
      "train loss:0.22522894271432364\n",
      "train loss:0.14078742823300275\n",
      "train loss:0.19853516716320865\n",
      "train loss:0.09999983291628778\n",
      "train loss:0.24499038653511448\n",
      "train loss:0.22408132450796792\n",
      "train loss:0.2545347510831412\n",
      "train loss:0.2885103227049692\n",
      "train loss:0.2884369436022807\n",
      "train loss:0.2273027938006031\n",
      "train loss:0.21786179928804633\n",
      "train loss:0.2370319003257631\n",
      "train loss:0.1589930668600309\n",
      "train loss:0.07864442629989767\n",
      "train loss:0.13425031201568718\n",
      "train loss:0.15060798559637834\n",
      "train loss:0.17051824104352428\n",
      "train loss:0.17028024062955385\n",
      "train loss:0.3665039866650273\n",
      "train loss:0.1579491307217808\n",
      "train loss:0.2074499987411731\n",
      "train loss:0.18842951102630984\n",
      "train loss:0.1966750320029581\n",
      "=== epoch:6, train acc:0.918, test acc:0.905 ===\n",
      "train loss:0.26408391608912485\n",
      "train loss:0.14703610452374322\n",
      "train loss:0.2573808153156278\n",
      "train loss:0.27276233658028204\n",
      "train loss:0.10843512497663892\n",
      "train loss:0.15918095461223503\n",
      "train loss:0.219527328576822\n",
      "train loss:0.14293832775698545\n",
      "train loss:0.2023691050801823\n",
      "train loss:0.13819535259203783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.17170562375427376\n",
      "train loss:0.2389976074171121\n",
      "train loss:0.3026042088713802\n",
      "train loss:0.11340398251540011\n",
      "train loss:0.24459626510871194\n",
      "train loss:0.25712450405680387\n",
      "train loss:0.15101039063447902\n",
      "train loss:0.2678002135395573\n",
      "train loss:0.14582667259099982\n",
      "train loss:0.128074275137873\n",
      "train loss:0.18634401549531313\n",
      "train loss:0.13395405528142437\n",
      "train loss:0.19060367677427734\n",
      "train loss:0.26214667460540514\n",
      "train loss:0.27826775364910383\n",
      "train loss:0.2617081321460271\n",
      "train loss:0.17326423969673155\n",
      "train loss:0.1532724953082087\n",
      "train loss:0.10753226728463802\n",
      "train loss:0.12681194873275733\n",
      "train loss:0.08959188133121918\n",
      "train loss:0.09698300680427333\n",
      "train loss:0.16933774818286945\n",
      "train loss:0.18194631136718814\n",
      "train loss:0.19548434990688546\n",
      "train loss:0.21894361933459183\n",
      "train loss:0.11924612860760055\n",
      "train loss:0.28874671901477417\n",
      "train loss:0.18686017068957878\n",
      "train loss:0.18291792572778331\n",
      "train loss:0.18163674285037293\n",
      "train loss:0.1984343650020705\n",
      "train loss:0.14239068847017985\n",
      "train loss:0.19026431390572235\n",
      "train loss:0.16713573210817276\n",
      "train loss:0.08899745444591436\n",
      "train loss:0.29920792140789343\n",
      "train loss:0.16961352547986397\n",
      "train loss:0.2174519916907099\n",
      "train loss:0.15052682165335485\n",
      "=== epoch:7, train acc:0.929, test acc:0.916 ===\n",
      "train loss:0.2848181291666723\n",
      "train loss:0.2459562412678387\n",
      "train loss:0.11659605765041571\n",
      "train loss:0.12842393521313927\n",
      "train loss:0.08749574201766844\n",
      "train loss:0.21624258509589633\n",
      "train loss:0.11466285874161296\n",
      "train loss:0.20125168718633166\n",
      "train loss:0.18661041176086834\n",
      "train loss:0.11131720877311535\n",
      "train loss:0.2130501240613764\n",
      "train loss:0.11101814581931947\n",
      "train loss:0.18604112084319596\n",
      "train loss:0.16285508994683162\n",
      "train loss:0.1444855918675948\n",
      "train loss:0.15420111122774574\n",
      "train loss:0.16207168753466117\n",
      "train loss:0.11584644920870192\n",
      "train loss:0.13160735616472202\n",
      "train loss:0.18936949123662353\n",
      "train loss:0.18944473979795917\n",
      "train loss:0.18786828141489678\n",
      "train loss:0.13336184861863007\n",
      "train loss:0.17924407690840408\n",
      "train loss:0.2328349185673763\n",
      "train loss:0.22194806464668632\n",
      "train loss:0.14921458799550513\n",
      "train loss:0.09159869403183805\n",
      "train loss:0.1785266030348022\n",
      "train loss:0.1454872787995227\n",
      "train loss:0.15882539055468808\n",
      "train loss:0.09436699188421908\n",
      "train loss:0.050704377973462285\n",
      "train loss:0.07207340813794268\n",
      "train loss:0.07991102561781661\n",
      "train loss:0.14481266991993416\n",
      "train loss:0.16401072373663672\n",
      "train loss:0.11379770519534213\n",
      "train loss:0.1548983599784082\n",
      "train loss:0.29184702640172744\n",
      "train loss:0.1079550410000088\n",
      "train loss:0.103762549435363\n",
      "train loss:0.18935019747471935\n",
      "train loss:0.1833097715056282\n",
      "train loss:0.12088893808765601\n",
      "train loss:0.2745220945157614\n",
      "train loss:0.31438366728688716\n",
      "train loss:0.167718814772981\n",
      "train loss:0.09477288577599\n",
      "train loss:0.20621282579419223\n",
      "=== epoch:8, train acc:0.953, test acc:0.925 ===\n",
      "train loss:0.09602952751389698\n",
      "train loss:0.10182045626887769\n",
      "train loss:0.0962353791376776\n",
      "train loss:0.16150387546579498\n",
      "train loss:0.1300084594353466\n",
      "train loss:0.18149350273722567\n",
      "train loss:0.15038653333956448\n",
      "train loss:0.20298356415448157\n",
      "train loss:0.1523403649838887\n",
      "train loss:0.09006669892804119\n",
      "train loss:0.1573139833542297\n",
      "train loss:0.2039175960532626\n",
      "train loss:0.13528488107454106\n",
      "train loss:0.14718117480259024\n",
      "train loss:0.1243545695568629\n",
      "train loss:0.0990056671067322\n",
      "train loss:0.14532709132387572\n",
      "train loss:0.22118139165649442\n",
      "train loss:0.19799693353898867\n",
      "train loss:0.1418080441027774\n",
      "train loss:0.25578396186108654\n",
      "train loss:0.09619557756633444\n",
      "train loss:0.16324964103983533\n",
      "train loss:0.1936167720257532\n",
      "train loss:0.07735208645771757\n",
      "train loss:0.14787888536934812\n",
      "train loss:0.12130475544028789\n",
      "train loss:0.18546192849288065\n",
      "train loss:0.09691903913619836\n",
      "train loss:0.1898923302784529\n",
      "train loss:0.13107859664002638\n",
      "train loss:0.1941619908913386\n",
      "train loss:0.10556817815614167\n",
      "train loss:0.10116411896807755\n",
      "train loss:0.12502230935518896\n",
      "train loss:0.15064788405470378\n",
      "train loss:0.21255308685779403\n",
      "train loss:0.13107953280732004\n",
      "train loss:0.12431095174905363\n",
      "train loss:0.06870094582438802\n",
      "train loss:0.07608287134422871\n",
      "train loss:0.10825611388503079\n",
      "train loss:0.09461074518953871\n",
      "train loss:0.1247615639870746\n",
      "train loss:0.14305852009956152\n",
      "train loss:0.10493918081886879\n",
      "train loss:0.1148293636730109\n",
      "train loss:0.15693445264872605\n",
      "train loss:0.05886795394214988\n",
      "train loss:0.14327545674789485\n",
      "=== epoch:9, train acc:0.951, test acc:0.932 ===\n",
      "train loss:0.16811968161725507\n",
      "train loss:0.10031704323950365\n",
      "train loss:0.06133959044378243\n",
      "train loss:0.14840931286908857\n",
      "train loss:0.15536773798123904\n",
      "train loss:0.0734668298335421\n",
      "train loss:0.1464318467548809\n",
      "train loss:0.1320943901349303\n",
      "train loss:0.15138422471595403\n",
      "train loss:0.1115874132952139\n",
      "train loss:0.18614650211627662\n",
      "train loss:0.07694008547945753\n",
      "train loss:0.11584776181141955\n",
      "train loss:0.14081887673306545\n",
      "train loss:0.07132336024149276\n",
      "train loss:0.15246647239625497\n",
      "train loss:0.09221435144202429\n",
      "train loss:0.08968293639527998\n",
      "train loss:0.05966084139081839\n",
      "train loss:0.10410257314363813\n",
      "train loss:0.11715942245163107\n",
      "train loss:0.05822364552916936\n",
      "train loss:0.10294525701374471\n",
      "train loss:0.12194983776734038\n",
      "train loss:0.09626804443894821\n",
      "train loss:0.10460518225138155\n",
      "train loss:0.06593538082647243\n",
      "train loss:0.0837229070931031\n",
      "train loss:0.16323162774429492\n",
      "train loss:0.05695633915557102\n",
      "train loss:0.09697287564972773\n",
      "train loss:0.09265128677935275\n",
      "train loss:0.1295132965784066\n",
      "train loss:0.035215076021157816\n",
      "train loss:0.1472382240618917\n",
      "train loss:0.07248476329098694\n",
      "train loss:0.09577704277455658\n",
      "train loss:0.11476176161983288\n",
      "train loss:0.08656758840714067\n",
      "train loss:0.055591125793049485\n",
      "train loss:0.16809337151372059\n",
      "train loss:0.06057757855434423\n",
      "train loss:0.21291489891503962\n",
      "train loss:0.0962607130188929\n",
      "train loss:0.09283382181934041\n",
      "train loss:0.08562682203377277\n",
      "train loss:0.0464355858806637\n",
      "train loss:0.11542550941203907\n",
      "train loss:0.16826373664568015\n",
      "train loss:0.07741426407298382\n",
      "=== epoch:10, train acc:0.952, test acc:0.933 ===\n",
      "train loss:0.23995212185624587\n",
      "train loss:0.08913832190037903\n",
      "train loss:0.038465226379156905\n",
      "train loss:0.09467182446510192\n",
      "train loss:0.06411347332674482\n",
      "train loss:0.11438911029705491\n",
      "train loss:0.24513857527130642\n",
      "train loss:0.053048348739478744\n",
      "train loss:0.04022565639602119\n",
      "train loss:0.14886819953722324\n",
      "train loss:0.08027995338736242\n",
      "train loss:0.1185837261022357\n",
      "train loss:0.08473430209645388\n",
      "train loss:0.07419069545177609\n",
      "train loss:0.057335996808278195\n",
      "train loss:0.05525995408039396\n",
      "train loss:0.05374152646405909\n",
      "train loss:0.057559627016682116\n",
      "train loss:0.07318602862555205\n",
      "train loss:0.28699246612329277\n",
      "train loss:0.0693060008998484\n",
      "train loss:0.08796891471431335\n",
      "train loss:0.1793303985155504\n",
      "train loss:0.1956956305618791\n",
      "train loss:0.15559273875984728\n",
      "train loss:0.09446942263154819\n",
      "train loss:0.14326678268208157\n",
      "train loss:0.08728997464818374\n",
      "train loss:0.10254794823435606\n",
      "train loss:0.11486260813341198\n",
      "train loss:0.08614110940538158\n",
      "train loss:0.0697987089642216\n",
      "train loss:0.04187873517011359\n",
      "train loss:0.13394200508342644\n",
      "train loss:0.13758585648674096\n",
      "train loss:0.06574118758885533\n",
      "train loss:0.0527445759962468\n",
      "train loss:0.14590343289766744\n",
      "train loss:0.11823408911430841\n",
      "train loss:0.13958883166412\n",
      "train loss:0.08343112162366148\n",
      "train loss:0.06729201614837842\n",
      "train loss:0.18350303732516468\n",
      "train loss:0.047815383038924006\n",
      "train loss:0.08208113443811504\n",
      "train loss:0.04342376303274614\n",
      "train loss:0.06640811022454962\n",
      "train loss:0.08629907174093697\n",
      "train loss:0.08492515990123367\n",
      "train loss:0.07295091356576709\n",
      "=== epoch:11, train acc:0.968, test acc:0.942 ===\n",
      "train loss:0.07108101405327442\n",
      "train loss:0.11796159757515223\n",
      "train loss:0.10496596654491157\n",
      "train loss:0.09835482011638211\n",
      "train loss:0.04923472880505234\n",
      "train loss:0.058013409417366546\n",
      "train loss:0.05901145471596823\n",
      "train loss:0.1671816285472786\n",
      "train loss:0.07699215509405974\n",
      "train loss:0.12119288001493808\n",
      "train loss:0.10380942330254922\n",
      "train loss:0.0965351009515524\n",
      "train loss:0.08834449105973827\n",
      "train loss:0.055347252142565966\n",
      "train loss:0.07515402787928878\n",
      "train loss:0.11579574954463068\n",
      "train loss:0.04971667974193217\n",
      "train loss:0.10139623258395605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03690786888192541\n",
      "train loss:0.09000000972048246\n",
      "train loss:0.10432401190260648\n",
      "train loss:0.07819698205926646\n",
      "train loss:0.06907932990750332\n",
      "train loss:0.08631141030378674\n",
      "train loss:0.07898263064390904\n",
      "train loss:0.03547872524233215\n",
      "train loss:0.10673937136381158\n",
      "train loss:0.11395283399847585\n",
      "train loss:0.09326101547464172\n",
      "train loss:0.05018038535533121\n",
      "train loss:0.03833308832355962\n",
      "train loss:0.10208973443231568\n",
      "train loss:0.05488701369657993\n",
      "train loss:0.07124380333644866\n",
      "train loss:0.04707552275781133\n",
      "train loss:0.07553510570184337\n",
      "train loss:0.06473427500835544\n",
      "train loss:0.2242698668692929\n",
      "train loss:0.09390335708464839\n",
      "train loss:0.06373264944381009\n",
      "train loss:0.05985710017282558\n",
      "train loss:0.06670265383470188\n",
      "train loss:0.0437268507647047\n",
      "train loss:0.10103053006623157\n",
      "train loss:0.06307471210713143\n",
      "train loss:0.15078438695563692\n",
      "train loss:0.06377892638521955\n",
      "train loss:0.07833249489538162\n",
      "train loss:0.07602370688756617\n",
      "train loss:0.03902175931934234\n",
      "=== epoch:12, train acc:0.972, test acc:0.946 ===\n",
      "train loss:0.0530876651072065\n",
      "train loss:0.09499512006735843\n",
      "train loss:0.09581043747484674\n",
      "train loss:0.055020006195278526\n",
      "train loss:0.030049492454610523\n",
      "train loss:0.051308316060888016\n",
      "train loss:0.177761203928567\n",
      "train loss:0.03769595289916938\n",
      "train loss:0.04749231931260687\n",
      "train loss:0.12782469387935627\n",
      "train loss:0.07611297448789796\n",
      "train loss:0.05521470790428187\n",
      "train loss:0.035463515241887844\n",
      "train loss:0.18728918200317626\n",
      "train loss:0.06216845211069949\n",
      "train loss:0.056654935470643035\n",
      "train loss:0.05681770337194343\n",
      "train loss:0.18689995295615622\n",
      "train loss:0.05044096818929689\n",
      "train loss:0.11470711658575816\n",
      "train loss:0.09161636231666793\n",
      "train loss:0.08801281521630389\n",
      "train loss:0.06328603009196508\n",
      "train loss:0.05330569104614366\n",
      "train loss:0.10799710788586843\n",
      "train loss:0.033193916991771974\n",
      "train loss:0.04009383982699127\n",
      "train loss:0.05344970438126996\n",
      "train loss:0.08845499961988056\n",
      "train loss:0.07560955592745872\n",
      "train loss:0.0632442685843953\n",
      "train loss:0.13432323816561506\n",
      "train loss:0.08843825520298328\n",
      "train loss:0.01961470504013183\n",
      "train loss:0.03898064089210737\n",
      "train loss:0.046349092269204535\n",
      "train loss:0.048865862038187\n",
      "train loss:0.06791132992478074\n",
      "train loss:0.023134151061410976\n",
      "train loss:0.03613074763682774\n",
      "train loss:0.11377799888793842\n",
      "train loss:0.1011829498343212\n",
      "train loss:0.09629292651629401\n",
      "train loss:0.07395368802847774\n",
      "train loss:0.10421105737158003\n",
      "train loss:0.08876553309846338\n",
      "train loss:0.11616623790075838\n",
      "train loss:0.06885979569757787\n",
      "train loss:0.06186098433007667\n",
      "train loss:0.13213455919658454\n",
      "=== epoch:13, train acc:0.974, test acc:0.949 ===\n",
      "train loss:0.050317278176069305\n",
      "train loss:0.05145123549464838\n",
      "train loss:0.07043272731324894\n",
      "train loss:0.079007682765792\n",
      "train loss:0.04318874178736062\n",
      "train loss:0.04794707536160187\n",
      "train loss:0.0679394051636827\n",
      "train loss:0.02945086734845684\n",
      "train loss:0.09334528794388211\n",
      "train loss:0.11643795194180412\n",
      "train loss:0.04112007222425236\n",
      "train loss:0.03075792594174788\n",
      "train loss:0.09195013901430615\n",
      "train loss:0.039764248153521996\n",
      "train loss:0.03920928015643379\n",
      "train loss:0.05461349721619093\n",
      "train loss:0.05697831041917028\n",
      "train loss:0.05686977672665958\n",
      "train loss:0.06401466740504745\n",
      "train loss:0.07475039999119089\n",
      "train loss:0.06663774595917504\n",
      "train loss:0.03537818760984932\n",
      "train loss:0.04801284144274165\n",
      "train loss:0.07290101108430558\n",
      "train loss:0.0729911411832328\n",
      "train loss:0.026121604237588813\n",
      "train loss:0.046205068537048255\n",
      "train loss:0.03277618258861901\n",
      "train loss:0.020517796088385283\n",
      "train loss:0.0830782807873193\n",
      "train loss:0.05205168498282287\n",
      "train loss:0.022292218614801306\n",
      "train loss:0.06746557908834531\n",
      "train loss:0.12176772950688765\n",
      "train loss:0.06266590225688543\n",
      "train loss:0.04619363573283885\n",
      "train loss:0.12420327596686556\n",
      "train loss:0.06701552628385996\n",
      "train loss:0.0622339798181433\n",
      "train loss:0.10694266191178053\n",
      "train loss:0.0395255184387818\n",
      "train loss:0.0549006143398019\n",
      "train loss:0.047394009424015025\n",
      "train loss:0.06290012668566364\n",
      "train loss:0.03513894399107201\n",
      "train loss:0.02749092080667227\n",
      "train loss:0.049854958016993335\n",
      "train loss:0.08676621756072214\n",
      "train loss:0.06711110674204926\n",
      "train loss:0.03152933280372301\n",
      "=== epoch:14, train acc:0.975, test acc:0.949 ===\n",
      "train loss:0.07560511084362163\n",
      "train loss:0.07317732552713074\n",
      "train loss:0.08080645608470022\n",
      "train loss:0.053642178076043344\n",
      "train loss:0.04410091189646104\n",
      "train loss:0.029297945477500805\n",
      "train loss:0.07446038369831298\n",
      "train loss:0.03191479651812979\n",
      "train loss:0.08472652275857914\n",
      "train loss:0.07338247527362272\n",
      "train loss:0.02520764824722664\n",
      "train loss:0.05368055243178562\n",
      "train loss:0.04101169217005708\n",
      "train loss:0.04868985035419397\n",
      "train loss:0.059289627171190865\n",
      "train loss:0.09232707839979679\n",
      "train loss:0.03186004161677554\n",
      "train loss:0.021204264421992122\n",
      "train loss:0.015578737178698714\n",
      "train loss:0.04522402062673905\n",
      "train loss:0.028920102632925337\n",
      "train loss:0.07350013344924675\n",
      "train loss:0.031705366993811726\n",
      "train loss:0.058187131600751424\n",
      "train loss:0.044521725797204745\n",
      "train loss:0.04108923122718183\n",
      "train loss:0.06450703988087256\n",
      "train loss:0.018644607756478356\n",
      "train loss:0.016651722878563445\n",
      "train loss:0.027806103812491347\n",
      "train loss:0.04139002932260206\n",
      "train loss:0.03457212046218357\n",
      "train loss:0.11623080960828354\n",
      "train loss:0.03925589752921681\n",
      "train loss:0.010618838411060231\n",
      "train loss:0.026825138113507122\n",
      "train loss:0.0225721077292461\n",
      "train loss:0.011416651175009911\n",
      "train loss:0.015041377335816017\n",
      "train loss:0.05231682611001923\n",
      "train loss:0.04906136910285875\n",
      "train loss:0.0953598318314684\n",
      "train loss:0.02570548806185102\n",
      "train loss:0.02419877095026173\n",
      "train loss:0.04344873269253167\n",
      "train loss:0.023445881784997263\n",
      "train loss:0.038464727865995016\n",
      "train loss:0.12382876075228401\n",
      "train loss:0.061235302532716626\n",
      "train loss:0.058683733461695996\n",
      "=== epoch:15, train acc:0.977, test acc:0.947 ===\n",
      "train loss:0.025422884537999582\n",
      "train loss:0.032780392869997045\n",
      "train loss:0.05931418399321548\n",
      "train loss:0.051131946721697075\n",
      "train loss:0.06495941465409942\n",
      "train loss:0.02403003292957577\n",
      "train loss:0.028106175211766678\n",
      "train loss:0.04471574777829405\n",
      "train loss:0.02753322072133962\n",
      "train loss:0.022149002683657718\n",
      "train loss:0.04645467913658545\n",
      "train loss:0.0315491369886827\n",
      "train loss:0.05669691532937502\n",
      "train loss:0.026475756980767314\n",
      "train loss:0.03976015933193585\n",
      "train loss:0.038087952399549624\n",
      "train loss:0.05032797977414667\n",
      "train loss:0.19379372511575976\n",
      "train loss:0.07569874290694831\n",
      "train loss:0.01602639136046314\n",
      "train loss:0.027360387307952344\n",
      "train loss:0.01602140850285507\n",
      "train loss:0.04860262481963163\n",
      "train loss:0.02214042415812299\n",
      "train loss:0.023385969494096846\n",
      "train loss:0.04193488083785743\n",
      "train loss:0.023847309950933638\n",
      "train loss:0.025480881364589723\n",
      "train loss:0.05906299648303918\n",
      "train loss:0.0725697393534083\n",
      "train loss:0.05460419522736545\n",
      "train loss:0.07331693466976445\n",
      "train loss:0.02512111150315092\n",
      "train loss:0.04684279218688386\n",
      "train loss:0.03257490226775462\n",
      "train loss:0.04037968813850066\n",
      "train loss:0.035395694843507\n",
      "train loss:0.06860307446463448\n",
      "train loss:0.05042808710507634\n",
      "train loss:0.06165374310191693\n",
      "train loss:0.04812763350198652\n",
      "train loss:0.019268299956413534\n",
      "train loss:0.029372029744097375\n",
      "train loss:0.013805139960611577\n",
      "train loss:0.110426757380467\n",
      "train loss:0.04034622779959154\n",
      "train loss:0.05897368839176055\n",
      "train loss:0.06285072428520343\n",
      "train loss:0.023183457218035497\n",
      "train loss:0.022827250527075144\n",
      "=== epoch:16, train acc:0.982, test acc:0.96 ===\n",
      "train loss:0.028196633590431328\n",
      "train loss:0.026736938088593464\n",
      "train loss:0.062094033318015766\n",
      "train loss:0.05212295677937526\n",
      "train loss:0.024940943486774052\n",
      "train loss:0.051082418173081956\n",
      "train loss:0.039011371824872576\n",
      "train loss:0.0794702159806346\n",
      "train loss:0.019709318189763146\n",
      "train loss:0.045685660832305436\n",
      "train loss:0.05427592771062391\n",
      "train loss:0.03146499103469639\n",
      "train loss:0.02101962763639992\n",
      "train loss:0.04462501798713004\n",
      "train loss:0.015414897942592763\n",
      "train loss:0.08732963624319183\n",
      "train loss:0.022113433893633152\n",
      "train loss:0.0752936805654389\n",
      "train loss:0.02265104364302262\n",
      "train loss:0.037000090506351946\n",
      "train loss:0.015337471546858817\n",
      "train loss:0.06780199361330523\n",
      "train loss:0.07787670133475794\n",
      "train loss:0.026864506249378216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04266544589369901\n",
      "train loss:0.020517719318000532\n",
      "train loss:0.030295013148491347\n",
      "train loss:0.031803257235390285\n",
      "train loss:0.018954484786708987\n",
      "train loss:0.0493304480871366\n",
      "train loss:0.029345113752565433\n",
      "train loss:0.026574189781847873\n",
      "train loss:0.021287891819535165\n",
      "train loss:0.0239422107116167\n",
      "train loss:0.04950704871029379\n",
      "train loss:0.030181660037827625\n",
      "train loss:0.021395743922893652\n",
      "train loss:0.07669206659740677\n",
      "train loss:0.12310324657231538\n",
      "train loss:0.0242658509614248\n",
      "train loss:0.031526640516072596\n",
      "train loss:0.03152337392764629\n",
      "train loss:0.046668939928866456\n",
      "train loss:0.03381496124022909\n",
      "train loss:0.03646658798582071\n",
      "train loss:0.026414517317157937\n",
      "train loss:0.04443043234783775\n",
      "train loss:0.01901654751506737\n",
      "train loss:0.025922224796286898\n",
      "train loss:0.011692508948007751\n",
      "=== epoch:17, train acc:0.99, test acc:0.954 ===\n",
      "train loss:0.029078750188832925\n",
      "train loss:0.023478714261374493\n",
      "train loss:0.039196657643961734\n",
      "train loss:0.04293139647844635\n",
      "train loss:0.021979659498307602\n",
      "train loss:0.02274386028904415\n",
      "train loss:0.017773219408013262\n",
      "train loss:0.03197209249463566\n",
      "train loss:0.05733675698097214\n",
      "train loss:0.028772263045608867\n",
      "train loss:0.03646598178915875\n",
      "train loss:0.04646350041443456\n",
      "train loss:0.013132524599953968\n",
      "train loss:0.02352362416536603\n",
      "train loss:0.04989798254483991\n",
      "train loss:0.02262761522548405\n",
      "train loss:0.01596067034380179\n",
      "train loss:0.018666638623183893\n",
      "train loss:0.07276823085642987\n",
      "train loss:0.013984818223343536\n",
      "train loss:0.04929126511433459\n",
      "train loss:0.08318433526098136\n",
      "train loss:0.008501176826898391\n",
      "train loss:0.05225894214444939\n",
      "train loss:0.034114192805204765\n",
      "train loss:0.014957352593299611\n",
      "train loss:0.0177952852694124\n",
      "train loss:0.01802869401164639\n",
      "train loss:0.023147619853492403\n",
      "train loss:0.031389316756378835\n",
      "train loss:0.04735531250009575\n",
      "train loss:0.024617993249153428\n",
      "train loss:0.022128707496676534\n",
      "train loss:0.03466512393990927\n",
      "train loss:0.09803847526308525\n",
      "train loss:0.013279945602518519\n",
      "train loss:0.011814134290278912\n",
      "train loss:0.03841734451814836\n",
      "train loss:0.028128060090066968\n",
      "train loss:0.050170259732475776\n",
      "train loss:0.035263113909863406\n",
      "train loss:0.019496027503806444\n",
      "train loss:0.02477987754362395\n",
      "train loss:0.02189017979957388\n",
      "train loss:0.04941783846873039\n",
      "train loss:0.0509058754866614\n",
      "train loss:0.049808873477265384\n",
      "train loss:0.022084790798193983\n",
      "train loss:0.026125036836760915\n",
      "train loss:0.04921696576207564\n",
      "=== epoch:18, train acc:0.989, test acc:0.955 ===\n",
      "train loss:0.013280505025021911\n",
      "train loss:0.03650674454395849\n",
      "train loss:0.07010700120325176\n",
      "train loss:0.03190413214668598\n",
      "train loss:0.024452862975725687\n",
      "train loss:0.056584610839761094\n",
      "train loss:0.017852358569431553\n",
      "train loss:0.033235389932179386\n",
      "train loss:0.036224233830113535\n",
      "train loss:0.01824782936733072\n",
      "train loss:0.013026469119045455\n",
      "train loss:0.1046635898999446\n",
      "train loss:0.042202004386440936\n",
      "train loss:0.05536103770436578\n",
      "train loss:0.06500684473615113\n",
      "train loss:0.029297440990376063\n",
      "train loss:0.02050042037557108\n",
      "train loss:0.03224988902300997\n",
      "train loss:0.04115632689650421\n",
      "train loss:0.02187636613903786\n",
      "train loss:0.0476274935301058\n",
      "train loss:0.020315261880113344\n",
      "train loss:0.02185114086666048\n",
      "train loss:0.030782177194819932\n",
      "train loss:0.00945362182310514\n",
      "train loss:0.0233186994560486\n",
      "train loss:0.024226883161186805\n",
      "train loss:0.015916515862898276\n",
      "train loss:0.031804819750836856\n",
      "train loss:0.011411326534621824\n",
      "train loss:0.043837962726212806\n",
      "train loss:0.018363133596641393\n",
      "train loss:0.04217694309959616\n",
      "train loss:0.026253418465294113\n",
      "train loss:0.013119910969429209\n",
      "train loss:0.03952482352637319\n",
      "train loss:0.04976294278142139\n",
      "train loss:0.03453726332781634\n",
      "train loss:0.03389238643931789\n",
      "train loss:0.025042570748241188\n",
      "train loss:0.038079464363617324\n",
      "train loss:0.04414874409442786\n",
      "train loss:0.030445499823443065\n",
      "train loss:0.020738210401617198\n",
      "train loss:0.010957263827760973\n",
      "train loss:0.010943627638853717\n",
      "train loss:0.05063435474452783\n",
      "train loss:0.020150554820338562\n",
      "train loss:0.017263177718032884\n",
      "train loss:0.03744498111432936\n",
      "=== epoch:19, train acc:0.992, test acc:0.953 ===\n",
      "train loss:0.04076714515745432\n",
      "train loss:0.03315112633446328\n",
      "train loss:0.02861694054621627\n",
      "train loss:0.024485146257624574\n",
      "train loss:0.039553164621604096\n",
      "train loss:0.03937727200089178\n",
      "train loss:0.022145575097423986\n",
      "train loss:0.020856891888374807\n",
      "train loss:0.021498669920949228\n",
      "train loss:0.009386502611876903\n",
      "train loss:0.022134711772664947\n",
      "train loss:0.027394550689212382\n",
      "train loss:0.013597674428342582\n",
      "train loss:0.03364693196380997\n",
      "train loss:0.014168448051740168\n",
      "train loss:0.017435362537931348\n",
      "train loss:0.016675974137025408\n",
      "train loss:0.015487562175703047\n",
      "train loss:0.008070514979630935\n",
      "train loss:0.0282707166994146\n",
      "train loss:0.017432135355611077\n",
      "train loss:0.010994683036360033\n",
      "train loss:0.031140666672839577\n",
      "train loss:0.05151455550052055\n",
      "train loss:0.011576318471237683\n",
      "train loss:0.009820538105003768\n",
      "train loss:0.013587649270639606\n",
      "train loss:0.028656908438354556\n",
      "train loss:0.017121571069810618\n",
      "train loss:0.011136478187666578\n",
      "train loss:0.008178239490172325\n",
      "train loss:0.017988448034520023\n",
      "train loss:0.03419283983282842\n",
      "train loss:0.006511674806452515\n",
      "train loss:0.024654647043786403\n",
      "train loss:0.024811150143412314\n",
      "train loss:0.02822216921618269\n",
      "train loss:0.025913587133982837\n",
      "train loss:0.017231270520080694\n",
      "train loss:0.01421324378893261\n",
      "train loss:0.028435127904447407\n",
      "train loss:0.021746080134631206\n",
      "train loss:0.023567435245163835\n",
      "train loss:0.07667788740647627\n",
      "train loss:0.027082541193039104\n",
      "train loss:0.02594849408563804\n",
      "train loss:0.020200133972736484\n",
      "train loss:0.006919859390751875\n",
      "train loss:0.016627275826829088\n",
      "train loss:0.018420749632023222\n",
      "=== epoch:20, train acc:0.991, test acc:0.958 ===\n",
      "train loss:0.022443839941555962\n",
      "train loss:0.024259047096168707\n",
      "train loss:0.03541829421382496\n",
      "train loss:0.015679692509334625\n",
      "train loss:0.010817845331604865\n",
      "train loss:0.026516219069528037\n",
      "train loss:0.022439675885583554\n",
      "train loss:0.025272037674842544\n",
      "train loss:0.008784213561521585\n",
      "train loss:0.012703585993528109\n",
      "train loss:0.014014573628812843\n",
      "train loss:0.018122265904246714\n",
      "train loss:0.042017905505389634\n",
      "train loss:0.017998153653573077\n",
      "train loss:0.004665780479361379\n",
      "train loss:0.01730952591238074\n",
      "train loss:0.01167838649660479\n",
      "train loss:0.012014514003215983\n",
      "train loss:0.012845318707474327\n",
      "train loss:0.008279131795091845\n",
      "train loss:0.03291572591428648\n",
      "train loss:0.04224264140230081\n",
      "train loss:0.017369533001824554\n",
      "train loss:0.0072335569807809984\n",
      "train loss:0.01848941395812537\n",
      "train loss:0.014806098870255899\n",
      "train loss:0.005149325277076328\n",
      "train loss:0.01275806107392536\n",
      "train loss:0.02037006082072792\n",
      "train loss:0.017676363473015242\n",
      "train loss:0.04209683224985029\n",
      "train loss:0.009460433037804212\n",
      "train loss:0.016693512772155885\n",
      "train loss:0.011406932376992416\n",
      "train loss:0.017702285161084515\n",
      "train loss:0.015373459660532076\n",
      "train loss:0.030446502223295133\n",
      "train loss:0.017838273300300637\n",
      "train loss:0.018913943814820637\n",
      "train loss:0.018202191044128128\n",
      "train loss:0.013867745722946823\n",
      "train loss:0.035024247977491596\n",
      "train loss:0.02359562747581061\n",
      "train loss:0.01753657256860638\n",
      "train loss:0.007774926278918209\n",
      "train loss:0.048901415289716664\n",
      "train loss:0.02655761912146899\n",
      "train loss:0.01796457630108401\n",
      "train loss:0.019388558638880317\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.956\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import *\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
